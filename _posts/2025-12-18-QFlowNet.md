---
layout: post
title:  "QFlowNet: Fast, Diverse, and Efficient Unitary Synthesis with Generative Flow Networks"
date:   2025-12-18 12:32 +0400
image:  QFlowNet-schematic.png
tags:   Quantum Computing
---

Unitary Synthesis, the decomposition of a unitary matrix into a sequence of quantum gates, is a fundamental challenge in quantum compilation. 
Prevailing reinforcement learning (RL) approaches are often hampered by sparse reward signals, which necessitate complex reward shaping or long training times, and typically converge to a single policy, lacking solution diversity. 
In this work, we propose QFlowNet, a novel framework that learns efficiently from sparse signals by pairing a Generative Flow Network (GFlowNet) with Transformers. Our approach addresses two key challenges. 
First, the GFlowNet framework is fundamentally designed to learn a diverse policy that samples solutions proportional to their reward, overcoming the single-solution limitation of RL while offering faster inference than other generative models like diffusion. 
Second, the Transformers act as a powerful encoder, capturing the non-local structure of unitary matrices and compressing a high-dimensional state into a dense latent representation for the policy network. 
Our agent achieves an overall success rate of 99.7% on a 3-qubit benchmark (lengths 1-12) and discovers a diverse set of compact circuits, establishing QFlowNet as an efficient and diverse paradigm for unitary synthesis.

{% highlight text %}
Algorithm 1: QFlowNet Training Loop

Input: Forward and backward parameters θ_F^1, θ_B^1
       Any GFlowNet loss function L_TB
       (optional) Experience replay buffer B

For t = 1 to N_iters do
    1. Sample a batch of trajectories {τ_k^(t)} from P_F(· | ·, θ_F^t)
    2. (optional) Update B with {τ_k^(t)}
    3. Update backward policy:
       θ_B^(t+1) = θ_B^t - γ_t * (1/K) * Σ ∇ L_TB(θ_B^t; τ_k^(t))
    4. (optional) Resample a batch of trajectories {τ_k^(t)} from B
    5. Update forward policy:
       θ_F^(t+1) = θ_F^t - η_t * (1/K) * Σ ∇ L_TB(θ_F^t; τ_k^(t), P_B(· | ·, θ_B^(t+1)))
End For
{% endhighlight %}

**Algorithm 1: QFlowNet Training Loop**

* **Input:** Forward and backward parameters $\theta_\mathrm{F}^1, \theta_\mathrm{B}^1$, any GFlowNet loss function $\mathcal{L}_{\text{TB}}$, *(optional)* experience replay buffer $\mathcal{B}$
* **For** $t = 1$ to $N_{\text{iters}}$ **do**
  * Sample a batch of trajectories $\left\{\tau_k^{(t)}\right\}_{k=1}^K$ from the forward policy $P_\mathrm{F}\left(\cdot \mid \cdot, \theta_\mathrm{F}^t\right)$
  * *(optional)* Update $\mathcal{B}$ with $\left\{\tau_k^{(t)}\right\}_{k=1}^K$
  * Update backward policy:
    $$\theta_\mathrm{B}^{t+1} = \theta_\mathrm{B}^t - \gamma_t \cdot \frac{1}{K} \sum_{k=1}^K \nabla_{\theta_\mathrm{B}^t} \mathcal{L}_{\text{TB}}\left(\theta_\mathrm{B}^t; \tau_k^{(t)}\right)$$
  * *(optional)* Resample a batch of trajectories $\left\{\tau_k^{(t)}\right\}_{k=1}^K$ from $\mathcal{B}$
  * Update forward policy:
    $$\theta_\mathrm{F}^{t+1} = \theta_\mathrm{F}^t - \eta_t \cdot \frac{1}{K} \sum_{k=1}^K \nabla_{\theta_\mathrm{F}^t} \mathcal{L}_{\text{TB}}\left(\theta_\mathrm{F}^t; \tau_k^{(t)}, P_\mathrm{B}\left(\cdot \mid \cdot, \theta_\mathrm{B}^{t+1}\right)\right)$$
* **End For**

[jekyll-docs]: https://inaekoo.github.io
[jekyll-gh]:   https://github.com/InaeKoo
