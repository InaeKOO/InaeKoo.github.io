---
layout: post
title:  "Adaptively Acquiring Shadows of a Quantum System from Very Few Measurements via RL"
date:   2025-12-30 12:10 +0900
image:  Adaptive.png
hide_header_image: true
tags:   Quantum Computing
---
![Adaptive](/images/Adaptive.png)

### *Adaptively Acquiring Shadows of a Quantum System from Very Few Measurements via RL*

> **"Don't measure randomly. Let AI learn *what* to measure."**

### ðŸ’¡ The Problem: The Measurement Bottleneck
Efficiently characterizing quantum many-body systems is a prerequisite for verifying quantum devices. Traditional tomography scales exponentially with system size ($d=2^n$). While randomized protocols like **Classical Shadows** circumvent this scaling, they rely on fixed, task-agnostic sampling distributions and often waste the measurement budget on uninformative subspaces.

### ðŸš€ Key Innovation: "Learning to Measure"
I propose a general-purpose **Reinforcement Learning framework** that treats the measurement process as a sequential decision-making problem. [cite_start]Unlike static protocols, our agent dynamically selects the optimal Pauli basis ($X, Y, Z$) conditioned on the history of previous measurement outcomes.

* **Architecture:** A **Transformer-based policy network** captures correlations across qubits and measurement history to predict the most discriminative basis in real-time.
* **Method:** The agent operates in a feedback loop, updating its internal belief state with every new bit outcome to maximize classification accuracy with minimal sample complexity.

### Why It Matters
This framework shifts the paradigm from random sampling to **adaptive, task-oriented acquisition**.
* **Sample Efficiency:** Achieves high accuracy in classifying entangled states (GHZ, W) and diagnosing noise with significantly fewer samples compared to random baselines.
* **Versatility:** The architecture generalizes to diverse tasks, including noise diagnosis, purity certification, and entanglement characterization.
